## 目录
- [目录](#目录)
- [1. 引言与核心思想](#1-引言与核心思想)
- [2. 一般交替极小化方法](#2-一般交替极小化方法)
  - [2.1 算法描述](#21-算法描述)
  - [2.2 收敛性定理 (连续可微)](#22-收敛性定理-连续可微)
  - [2.3 反例：连续不可微](#23-反例连续不可微)
  - [2.4 坐标轮换最小值点](#24-坐标轮换最小值点)
  - [2.5 收敛性定理 (连续不可微)](#25-收敛性定理-连续不可微)
  - [2.6 坐标轮换最小值点与稳定点的关系](#26-坐标轮换最小值点与稳定点的关系)
  - [2.7 反例：子问题最优解不唯一](#27-反例子问题最优解不唯一)
- [3. 和式凸优化交替极小化方法](#3-和式凸优化交替极小化方法)
  - [3.1 问题形式与算法](#31-问题形式与算法)
  - [3.2 全局收敛性定理](#32-全局收敛性定理)
  - [3.3 收敛速率定理](#33-收敛速率定理)
- [4. 二分块交替极小化方法 (Two-Block AMM)](#4-二分块交替极小化方法-two-block-amm)
  - [4.1 问题形式](#41-问题形式)
  - [4.2 算法描述](#42-算法描述)
  - [4.3 收敛速率定理](#43-收敛速率定理)
  - [4.4 与多分块收敛性比较](#44-与多分块收敛性比较)
- [5. 二分块线性化交替极小化方法 (Two-Block Linearized AMM)](#5-二分块线性化交替极小化方法-two-block-linearized-amm)
  - [5.1 问题形式](#51-问题形式)
  - [5.2 算法思想与迭代步骤](#52-算法思想与迭代步骤)
  - [5.3 收敛性定理](#53-收敛性定理)
- [6. 总结与复习要点](#6-总结与复习要点)

---

## 1. 引言与核心思想

**交替极小化方法 (Alternating Minimization Method, AMM)** 是一种优化策略，其核心思想是将复杂的多变量优化问题分解为一系列更简单、更小规模的子问题来求解 。

* **基本思路**：根据优化问题的结构将变量分块，然后依次对每组变量进行优化求解，而固定其他组的变量 。
* **战术**：“化整为零、各个击破” 。
* **目的**：降低问题规模、简化问题难度、提高算法效率 。

---

## 2. 一般交替极小化方法

考虑优化问题：
$$ \min_{x_{1}\in\mathbb{R}^{n_{1}},x_{2}\in\mathbb{R}^{n_{2}},\cdot\cdot\cdot,x_{s}\in\mathbb{R}^{n_{s}}}\Psi(x_{1},x_{2},\cdot\cdot\cdot,x_{s}) $$ AMM 以循环方式交替对 $s$ 个模块变量求最小 。

### 2.1 算法描述

**迭代过程**：
给定迭代点 $x^{k}=(x_{1}^{k},x_{2}^{k},\cdot\cdot\cdot,x_{s}^{k})$，通过依次对模块变量 $x_{1},x_{2},\cdot\cdot\cdot,x_{s}$ 求极小，得到新的迭代点 $x^{k+1}$ 。

**算法步骤总结**：
* **初始步**: 取 $x^{0}=(x_{1}^{0},x_{2}^{0},\cdot\cdot\cdot,x_{s}^{0})$ , $k=0$ 。
* **迭代步**: 对 $i=1,2,\cdot\cdot\cdot,s$, 依次求解子问题：
    $$ x_{i}^{k+1}=\arg\min_{x_{i}\in\mathbb{R}^{n_{i}}}\Psi(x_{1}^{k+1},x_{2}^{k+1},\cdot\cdot\cdot,x_{i-1}^{k+1},x_{i},x_{i+1}^{k},\cdot\cdot\cdot,x_{s}^{k}) $$ 

### 2.2 收敛性定理 (连续可微)

**定理**: 设目标函数 $\Psi:\mathbb{R}^{n}\rightarrow\mathbb{R}$ **连续可微**，目标函数**水平集有界**，且对任意的 $x\in\mathbb{R}^{n}$ 和 $i\in\{1,2,\cdot\cdot\cdot,s\}$，子问题 $min_{y\in\mathbb{R}^{n_{i}}}\Psi(x_{1},x_{2},\cdot\cdot\cdot,x_{i-1},y,x_{i+1},\cdot\cdot\cdot,x_{s})$ 有**唯一最优解**。则算法产生迭代点列的任一**聚点为优化问题的稳定点** 。

### 2.3 反例：连续不可微

考虑函数 $\min \Psi(x_{1},x_{2})=|3x_{1}+4x_{2}|+|x_{1}-2x_{2}|$ 。
这是一个连续凸函数，水平集有界，且对任一分量有唯一最优解 。

* 固定 $x_1 = -4\alpha$ (假设 $\alpha > 0$):
    $\Psi(-4\alpha, x_2) = |4x_2 - 12\alpha| + |-2x_2 - 4\alpha|$. 最优解为 $x_2 = 3\alpha$ 。
* 固定 $x_2 = 3\alpha$:
    $\Psi(x_1, 3\alpha) = |3x_1 + 12\alpha| + |x_1 - 6\alpha|$. 最优解为 $x_1 = -4\alpha$ 。

若初始点 $x_1$ 非零，例如取 $\alpha_0 \ne 0$ 使得 $x_1^0 = -4\alpha_0$。
第一次迭代：
1.  $x_2^1 = \arg\min_{x_2} \Psi(x_1^0, x_2) = 3\alpha_0$ (假设 $x_1^0=-4\alpha_0$)。
2.  $x_1^1 = \arg\min_{x_1} \Psi(x_1, x_2^1) = -4\alpha_0$。
算法在 $(-4\alpha_0, 3\alpha_0)$ 点停滞。
然而，该函数的唯一最小值点是 $x=0$。点 $(-4\alpha_0, 3\alpha_0)$ 既不是最小值点，也不是稳定点 (因为函数在 $(0,0)$ 以外不可微，稳定点的概念可能不适用，但它不是最小值点)。

这引出了一个问题：这个停滞点是什么性质的点？

### 2.4 坐标轮换最小值点

**定义**: 若 $x^{*}\in\mathbb{R}^{n}$ 满足
$$ \Psi(x^{*})\le\Psi(x_{1}^{*},\cdot\cdot\cdot,x_{i-1}^{*},x_{i},x_{i+1}^{*},\cdot\cdot\cdot,x_{s}^{*}) \quad , \forall i=1,2,\cdot\cdot\cdot,s, \quad x_{i}\in\mathbb{R}^{n_{i}} $$
则称 $x^{*}$ 为函数 $\Psi(x)$ 的**坐标轮换最小值点 (Coordinate-wise Minimum Point)** 。
* 这意味着 $x^*$ 对于每个变量块 $x_i^*$ 都是最优的，当其他变量块 $x_j^*$ ($j \ne i$) 固定时。
* 在上面的反例中，$(-4\alpha, 3\alpha)$ 就是一个坐标轮换最小值点 。

### 2.5 收敛性定理 (连续不可微)

**定理**: 设分块优化问题 $\min_{x_{1}\in\mathbb{R}^{n_{1}},\cdot\cdot\cdot,x_{s}\in\mathbb{R}^{n_{s}}}\Psi(x_{1},\cdot\cdot\cdot,x_{s})$ 的目标函数**下半连续**，**水平集有界**，且子问题 $\min_{y\in\mathbb{R}^{n_{i}}}\Psi(x_{1},\cdot\cdot\cdot,x_{i-1},y,x_{i+1},\cdot\cdot\cdot,x_{s})$ 有**唯一最优解**。则迭代点列的任一聚点为**坐标轮换最小值点** 。

* **下半连续 (Lower Semicontinuous)**: 一个函数是下半连续的，如果其水平集 $\{x | \Psi(x) \le \alpha\}$ 对于所有 $\alpha \in \mathbb{R}$ 都是闭集。
* **重要启示**: 对于连续不可微函数，AMM 收敛到的点可能只是坐标轮换最小值点，而不一定是全局最小值点或稳定点 。

### 2.6 坐标轮换最小值点与稳定点的关系

**问题**: 在什么情况下，坐标轮换最小值点是稳定点或最小值点？

**定理**: 对优化问题 $\min_{x\in\mathbb{R}^{n}}\Psi(x)=f(x)+g(x)$，其中：
* $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ **连续可微**。
* $g(x)=\sum_{i=1}^{s}g_{i}(x_{i})$，$g_{i}:\mathbb{R}^{n_{i}}\rightarrow\mathbb{R}$ **连续凸函数**。
在这种情况下，**坐标轮换极小值点等价于稳定点**。

* **注意**: 如果子问题的最优解不唯一，算法的收敛性可能无法保证。

### 2.7 反例：子问题最优解不唯一

考虑函数：
$$ f(x,y,z)=-xy-yz-zx+[x-1]_{+}^{2}+[-x-1]_{+}^{2}+[y-1]_{+}^{2}+[-y-1]_{+}^{2}+[z-1]_{+}^{2}+[-z-1]_{+}^{2} $$ 该函数连续可微。
子问题的最优解形式为：
$$ \arg\min_{x}f(x,y,z)=\begin{cases}sgn(y+z)(1+\frac{1}{2}|y+z|),&y+z\ne0,\\ [-1,1],&y+z=0.\end{cases} $$
(对 $y, z$ 类似)。当 $y+z=0$ 时，关于 $x$ 的子问题最优解不唯一 (为区间 $[-1,1]$)。

取 $\epsilon > 0$，以 $(-1-\epsilon, 1+\frac{1}{2}\epsilon, -1-\frac{1}{4}\epsilon)$ 为初始点，迭代6次后，若将初始点中的 $\epsilon$ 换成 $\frac{1}{64}\epsilon$，则算法产生的迭代点列会围绕以下6点循环：
$(1,1,-1), (1,-1,-1), (1,-1,1), (-1,-1,1), (-1,1,1), (-1,1,-1)$ [cite: 11, 12]。
这些聚点并非稳定点，也不是坐标轮换最小值点。

* **教训**: 子问题最优解的唯一性对于一般 AMM 的良好行为至关重要。

---

## 3. 和式凸优化交替极小化方法

考虑特殊的和式结构凸优化问题：
$$ \min_{x\in\mathbb{R}^{n}}\Psi(x)=f(x)+g(x) $$ 其中：
* $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ 是**连续可微的凸函数** 。
* $g(x)=\sum_{i=1}^{s}g_{i}(x_{i})$，$g_{i}:\mathbb{R}^{n_{i}}\rightarrow\mathbb{R}$ 是**连续凸函数** 。

### 3.1 问题形式与算法
算法与一般 AMM 相同，即依次对每个变量块 $x_i$ 进行最小化，求解：
$x_{i}^{k+1}=\arg\min_{x_{i}} f(x_{1}^{k+1},\dots,x_{i-1}^{k+1},x_{i},x_{i+1}^{k},\dots,x_{s}^{k}) + g_i(x_i)$

### 3.2 全局收敛性定理

**定理**: 对于上述和式凸优化问题，若目标函数**水平集有界**，则交替极小化方法产生的迭代点列的任一聚点为问题的**最优解** 。

* 在这种凸结构下，**无需子问题最优解唯一**的条件，算法就能保证全局收敛到最优解 。这是非常强大的结果。

### 3.3 收敛速率定理

**定理**: 设 $\min_{x\in\mathbb{R}^{n}}\Psi(x)=f(x)+g(x)$，其中：
* $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ 是连续可微的凸函数，且其**梯度函数 $\nabla f$ Lipschitz 连续**，常数为 $L_{f}$ 。
    ($||\nabla f(x) - \nabla f(y)|| \le L_f ||x-y||$)
* $g(x)=\sum_{i=1}^{s}g_{i}(x_{i})$，$g_{i}:\mathbb{R}^{n_{i}}\rightarrow\mathbb{R}$ 是**下半连续的凸函数** 。
* 目标函数**水平集有界** 。

则算法产生的迭代点列满足目标函数值的线性下降 (应为次线性，原文表述为线性下降，但速率项为 $O(1/k)$)：
$$ \Psi(x^{k})-\Psi^{*}\le \max\left\{\left(\frac{1}{2}\right)^{(k-1)/2}(\Psi(x^{0})-\Psi^{*}),\frac{8s^{2}L_{f}R_{\Psi(x^{0})}^{2}}{k-1}\right\} $$ 其中 $\Psi^{*}$ 是最优值，$R_{\Psi(x^{0})}=\max\{||x-x^{*}|| | \Psi(x)\le\Psi(x^{0})\}$ 是初始水平集直径的上界 。

* **收敛速率**: 该定理给出了 $O(1/k)$ 的次线性收敛速率（对于函数值）。

---

## 4. 二分块交替极小化方法 (Two-Block AMM)

这是 AMM 的一个重要特例，变量被分为两块 ($s=2$)。也常被称为**块坐标下降 (Block Coordinate Descent, BCD)** for two blocks。

### 4.1 问题形式

$$ \min_{x_{1}\in\mathbb{R}^{n_{1}},x_{2}\in\mathbb{R}^{n_{2}}}\{\Psi(x_{1},x_{2})=f(x_{1},x_{2})+g_{1}(x_{1})+g_{2}(x_{2})\} $$ 其中：
* $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ 是**连续可微的凸函数** ($n=n_1+n_2$) 。
* $\nabla_{i}f(x)$ (即 $\nabla_{x_i}f(x_1, x_2)$) 关于 $x_i$ **Lipschitz 连续**，常数为 $L_{i}$, $i=1,2$ 。
    ($||\nabla_{x_1}f(y, x_2) - \nabla_{x_1}f(z, x_2)|| \le L_1 ||y-z||$ 对固定的 $x_2$)
    ($||\nabla_{x_2}f(x_1, y) - \nabla_{x_2}f(x_1, z)|| \le L_2 ||y-z||$ 对固定的 $x_1$)
* $g_{i}:\mathbb{R}^{n_{i}}\rightarrow\mathbb{R}$ 是**下半连续的凸函数** 。

### 4.2 算法描述

**初始步**: 取 $x_{1}^{0}\in\mathbb{R}^{n_{1}}$，计算 $x_{2}^{0}=\arg\min_{x_{2}}f(x_{1}^{0},x_{2})+g_{2}(x_{2})$ 。
（注意：这里的初始化步骤先固定 $x_1^0$ 算 $x_2^0$，与一般迭代顺序略有不同，但本质上是选择一个起始块。）

**迭代步**: 依次计算
1.  $x_{1}^{k+1}=\arg\min_{x_{1}}f(x_{1},x_{2}^{k})+g_{1}(x_{1})$ 
2.  $x_{2}^{k+1}=\arg\min_{x_{2}}f(x_{1}^{k+1},x_{2})+g_{2}(x_{2})$ 

### 4.3 收敛速率定理

**定理**: 对于上述二分块问题，若目标函数**水平集有界**，则算法产生的迭代点列满足：
$$ \Psi(x^{k})-\Psi^{*}\le \max\left\{\left(\frac{1}{2}\right)^{(k-1)/2}(\Psi(x^{0})-\Psi^{*}),\frac{8 \min\{L_{1},L_{2}\}R_{\Psi(x^{0})}^{2}}{k-1}\right\} $$ 其中 $R_{\Psi(x^{0})}=\max\{||x-x^{*}|| | \Psi(x)\le\Psi(x^{0})\}$ 。

### 4.4 与多分块收敛性比较

* **多分块 ($s \ge 2$) 和式凸优化 AMM** [cite: 14, 18]：
    $f$ 的梯度 $\nabla f$ Lipschitz 连续，常数 $L_f$.
    $\Psi(x^{k})-\Psi^{*}\le \max\left\{\dots, \frac{8s^{2}L_{f}R_{\Psi(x^{0})}^{2}}{k-1}\right\}$ 。
* **二分块 ($s=2$) AMM** [cite: 15, 18]：
    $\nabla_{i}f(x)$ 关于 $x_i$ Lipschitz 连续，常数 $L_i$.
    $\Psi(x^{k})-\Psi^{*}\le \max\left\{\dots, \frac{8 \min\{L_{1},L_{2}\}R_{\Psi(x^{0})}^{2}}{k-1}\right\}$ 。

**观察**:
* 二分块情况下的速率常数依赖于 $\min\{L_1, L_2\}$，而多分块情况下依赖于全局 Lipschitz 常数 $L_f$ 和块数 $s$ ($s^2 L_f$) 。
* 如果 $s=2$，则 $s^2 L_f = 4 L_f$。 通常 $L_i \le L_f$。二分块的界可能更紧 (常数更小)。

---

## 5. 二分块线性化交替极小化方法 (Two-Block Linearized AMM)

当子问题 $ \min_{x_{1}}f(x_{1},x_{2}^{k})+g_{1}(x_{1}) $ 仍然难以求解时，可以考虑对 $f(x_1, x_2)$ 部分进行线性化。

### 5.1 问题形式

$$ \min_{x_{1}\in\mathbb{R}^{n_{1}},x_{2}\in\mathbb{R}^{n_{2}}}\{\Psi(x_{1},x_{2})=f(x_{1},x_{2})+g_{1}(x_{1})+g_{2}(x_{2})\} $$ 其中：
* $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ **连续可微** (不一定凸) 。
* $\nabla_{i}f(x)$ 关于 $x_i$ **一致 Lipschitz 连续**，常数为 $L_{i}$, $i=1,2$ 。
    * $||\nabla_{x_{1}}f(y,x_{2})-\nabla_{x_{1}}f(z,x_{2})||\le L_{1}||y-z||$, $\forall y,z\in\mathbb{R}^{n_{1}},x_{2}\in\mathbb{R}^{n_{2}}$ 
    * $||\nabla_{x_{2}}f(x_{1},y)-\nabla_{x_{2}}f(x_{1},z)||\le L_{2}||y-z||$, $\forall y,z\in\mathbb{R}^{n_{2}},x_{1}\in\mathbb{R}^{n_{1}}$ 
* $g_{i}:\mathbb{R}^{n_{i}}\rightarrow\mathbb{R}$ **下半连续** (不一定凸) 。

### 5.2 算法思想与迭代步骤

**思想**：
1.  将函数 $f(x_{1},x_{2})$ 在 $(x_{1}^{k},x_{2}^{k})$ 点关于 $x_{1}$ **线性化**，再添加**二次近端正则项 (proximal regularization term)**，然后关于 $x_{1}$ 求最小得到 $x_{1}^{k+1}$ 。
2.  再将函数 $f(x_{1},x_{2})$ 在 $(x_{1}^{k+1},x_{2}^{k})$ 点关于 $x_{2}$ **线性化**，添加二次近端正则项，然后关于 $x_{2}$ 求最小得到 $x_{2}^{k+1}$ 。

**迭代过程 (Proximal Alternating Linearized Minimization - PALM)**:

1.  **$x_1$-subproblem**:
    $$ x_{1}^{k+1}=\arg\min_{x_{1}\in\mathbb{R}^{n_{1}}}\left\{\langle\nabla_{x_{1}}f(x_{1}^{k},x_{2}^{k}), x_{1}-x_{1}^{k}\rangle+\frac{L_{k}^{1}}{2}||x_{1}-x_{1}^{k}||^{2}+g_{1}(x_{1})\right\} $$ 这里 $L_{k}^{1}=\gamma_{1}L_{1}$，其中 $\gamma_{1}>1$ 是一个步长参数 。
    这个子问题等价于求解 $g_1(x_1)$ 的近端算子 (proximal operator) 加上一个线性项：
    $$ x_{1}^{k+1} = \mathrm{prox}_{\frac{1}{L_k^1}g_1} \left( x_1^k - \frac{1}{L_k^1} \nabla_{x_1}f(x_1^k, x_2^k) \right) $$

2.  **$x_2$-subproblem**:
    $$ x_{2}^{k+1}=\arg\min_{x_{2}\in\mathbb{R}^{n_{2}}}\left\{\langle\nabla_{x_{2}}f(x_{1}^{k+1},x_{2}^{k}),x_{2}-x_{2}^{k}\rangle+\frac{L_{k}^{2}}{2}||x_{2}-x_{2}^{k}||^{2}+g_{2}(x_{2})\right\} $$ 这里 $L_{k}^{2}=\gamma_{2}L_{2}$，其中 $\gamma_{2}>1$ 。
    类似地：
    $$ x_{2}^{k+1} = \mathrm{prox}_{\frac{1}{L_k^2}g_2} \left( x_2^k - \frac{1}{L_k^2} \nabla_{x_2}f(x_1^{k+1}, x_2^k) \right) $$

* **优点**: 子问题通常更容易求解，特别是当 $g_i$ 的近端算子有闭式解或高效算法时。
* **参数选取**: $L_k^1, L_k^2$ (或 $\gamma_1, \gamma_2$) 的选择影响收敛性。通常要求 $L_k^1 > L_1$ 和 $L_k^2 > L_2$。

### 5.3 收敛性定理

**定理**: (二分块) 线性化近端点交替极小化方法产生的迭代点列的任一聚点为优化问题的**稳定点** 。
* 注意：这里收敛到稳定点，因为 $f$ 和 $g_i$ 不一定是凸的。如果它们是凸的，则可以得到更强的收敛结果（收敛到全局最优）。

---

## 6. 总结与复习要点

* **核心思想**：分而治之，将大问题分解为针对变量块的系列小问题 。
* **一般 AMM**:
    * 连续可微 + 水平集有界 + 子问题唯一解 $\implies$ 收敛到稳定点 。
    * 不可微时，可能仅收敛到坐标轮换最小值点，该点不一定是全局最优或稳定点。
    * 子问题解不唯一时，可能不收敛 。
* **和式凸优化 AMM ($f(x) + \sum g_i(x_i)$)**:
    * $f$ 连续可微凸， $g_i$ 连续凸 + 水平集有界 $\implies$ 收敛到全局最优解 (无需子问题唯一解) 。
    * 若 $\nabla f$ Lipschitz，则有 $O(1/k)$ 收敛速率 。
* **二分块 AMM ($f(x_1,x_2) + g_1(x_1) + g_2(x_2)$)**:
    * $f$ 连续可微凸， $g_i$ 下半连续凸，$\nabla_i f$ Lipschitz + 水平集有界 $\implies$ 收敛到全局最优，有 $O(1/k)$ 速率，常数可能优于多分块 [cite: 15, 17, 18]。
* **二分块线性化 AMM (PALM)**:
    * 通过线性化 $f$ 的部分并加近端项，简化子问题 [cite: 20, 21]。
    * $f$ 连续可微 (不一定凸)， $g_i$ 下半连续 (不一定凸)，$\nabla_i f$ 一致 Lipschitz $\implies$ 收敛到稳定点。

**何时选择哪种方法？**
* 如果问题结构是 $f(x) + \sum g_i(x_i)$ 且凸，直接用和式 AMM。
* 如果子问题容易精确求解，标准 AMM 是一个选择。
* 如果子问题求解困难，但 $f$ 的梯度易得，$g_i$ 的近端算子易求，则线性化 AMM (如 PALM) 是好选择。
* 二分块通常比多分块有更好的理论保证和实践表现。

